# ğŸ§  Emotion Detector â€” Facial Emotion Recognition in Real Time

**Reconoce emociones humanas en tiempo real usando IA y visiÃ³n por computadora.**  
Un proyecto construido con **TensorFlow.js**, **MediaPipe FaceMesh** y **React**, que combina *machine learning* y *front-end interactivo*.

---

## ğŸ“– DescripciÃ³n

**Emotion Detector** es un proyecto de detecciÃ³n de emociones faciales basado en aprendizaje profundo.  
El modelo fue entrenado con el dataset **FER2013**, procesando imÃ¡genes de rostros para clasificar emociones humanas en siete categorÃ­as:  

ğŸ˜ Neutral Â· ğŸ˜€ Feliz Â· ğŸ˜¢ Triste Â· ğŸ˜  Enojado Â· ğŸ˜® Sorprendido Â· ğŸ˜¨ Miedo Â· ğŸ¤¢ Asco  

El flujo principal combina dos componentes:

- **MediaPipe FaceMesh**: detecta 468 puntos (*landmarks*) en el rostro humano y los convierte en datos numÃ©ricos.  
- **TensorFlow.js**: recibe esos datos como tensores y predice la emociÃ³n correspondiente con un modelo de red neuronal entrenado en Python y exportado a formato `.json` para usarse en la web.

---

## âš™ï¸ TecnologÃ­as utilizadas

| Ãrea | TecnologÃ­as |
|------|-------------|
| Frontend | React + Vite |
| IA / ML | TensorFlow (Python) Â· TensorFlow.js |
| VisiÃ³n por computadora | MediaPipe FaceMesh |
| Preprocesamiento | NumPy Â· PIL (Python Imaging Library) |
| Backend opcional | Node.js (para despliegues locales o APIs auxiliares) |

---

## ğŸ§© CÃ³mo funciona

### Entrenamiento del modelo (Python)

- Se usÃ³ el dataset **FER2013** con imÃ¡genes de 48x48 pÃ­xeles en escala de grises.
- El modelo es una red neuronal densa con varias capas y dropout: `Dense(512) â†’ Dropout(0.3) â†’ Dense(256) â†’ Dropout(0.3) â†’ Dense(128) â†’ Dense(7, softmax)`.
- Se normalizaron los valores X/Y de los 468 landmarks usando StandardScaler antes del entrenamiento.
- Tras entrenar hasta 100 Ã©pocas con early stopping y guardado del mejor modelo (best_emotion_model.h5), se exportÃ³ con `model.export("emotion_model")` para su uso en TensorFlow.js.
- Durante el entrenamiento, la accuracy de validaciÃ³n oscilÃ³ entre aproximadamente `0.55â€“0.56` y la loss entre `1.15â€“1.17`, reflejando la dificultad de diferenciar algunas emociones similares.

### PredicciÃ³n en tiempo real (JavaScript)

- El navegador detecta tu rostro con **FaceMesh**.  
- Convierte los *landmarks* en un tensor (`tf.tensor2d`) del tamaÃ±o esperado por el modelo.  
- **TensorFlow.js** predice la emociÃ³n y la dibuja sobre el video en vivo.

---

## ğŸš€ CÃ³mo usarlo

### ğŸ§© 1. Clonar el repositorio

```bash
git clone https://github.com/maaquin/EmotionDetector.git
cd EmotionDetector
```

### âš™ï¸ 2. Instalar dependencias
```bash
npm install
```

### â–¶ï¸ 4. Ejecutar el proyecto
```bash
npm run dev
```
- Luego abre tu navegador en http://localhost:5173

## ğŸŒ Despliegue
- Enlace al proyecto desplegado: [emotion-detector](https://emotion-detector-iota.vercel.app)

---

## âœ¨ Notas
- **Compatibilidad del navegador:** El proyecto funciona mejor en navegadores modernos que soporten WebGL 2.0 y WebAssembly.
- **Rendimiento:** La predicciÃ³n en tiempo real puede variar segÃºn la cÃ¡mara y CPU/GPU del dispositivo. Para mejorar la velocidad se puede reducir la resoluciÃ³n del video o usar un modelo mÃ¡s ligero.
- **PrecisiÃ³n del modelo:** El modelo MLP entrenado con FER2013 alcanza una accuracy de test aproximada de `0.55â€“0.57` y una loss de `1.14â€“1.17`, lo que significa que puede confundirse entre emociones similares, especialmente con fear, disgust o surprise.
- **NormalizaciÃ³n de landmarks:** Los valores X/Y de los 468 puntos faciales fueron normalizados usando StandardScaler, lo que asegura que las coordenadas estÃ©n en la misma escala que durante el entrenamiento y mejora la consistencia de las predicciones en tiempo real.
- **Seguridad y privacidad:** Todo el procesamiento ocurre en el navegador; las imÃ¡genes de video no se envÃ­an a servidores externos.

---

## ğŸ“Œ Autor
- Luciano Maquin â€“ [@Maaquin](https://github.com/maaquin)